{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff4021b6",
   "metadata": {},
   "source": [
    "# first dataset Elon_musk tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b712e8f5",
   "metadata": {},
   "source": [
    "### ONE:\n",
    "1) Perform sentimental analysis on the Elon-musk tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9308a94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Libraries if not installed\n",
    "#%pip install spacy\n",
    "#!python -m spacy download en_core_web_md\n",
    "#!pip install wordcloud\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d4ea578",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "dataclass_transform() got an unexpected keyword argument 'field_specifiers'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mstring\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m \n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m imread\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\__init__.py:6\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# set library-specific custom warning handling before doing anything else\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m setup_default_warnings\n\u001b[0;32m      8\u001b[0m setup_default_warnings()  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# These are imported as part of the API\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\errors.py:2\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Literal\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mErrorsWithCodes\u001b[39;00m(\u001b[38;5;28mtype\u001b[39m):\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattribute__\u001b[39m(\u001b[38;5;28mself\u001b[39m, code):\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\compat.py:3\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"Helpers for Python and platform compatibility.\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mthinc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m copy_array\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcPickle\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\thinc\\__init__.py:5\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mabout\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m registry\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# fmt: off\u001b[39;00m\n\u001b[0;32m      9\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregistry\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__version__\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     12\u001b[0m ]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\thinc\\config.py:2\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcatalogue\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mconfection\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mconfection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Config, ConfigValidationError, Promise, VARIABLE_RE\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Decorator\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\confection\\__init__.py:10\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mconfigparser\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ParsingError\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpathlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydantic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseModel, create_model, ValidationError, Extra\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydantic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmain\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ModelMetaclass\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydantic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfields\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ModelField\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pydantic\\__init__.py:2\u001b[0m, in \u001b[0;36minit pydantic.__init__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pydantic\\dataclasses.py:47\u001b[0m, in \u001b[0;36minit pydantic.dataclasses\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pydantic\\main.py:121\u001b[0m, in \u001b[0;36minit pydantic.main\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: dataclass_transform() got an unexpected keyword argument 'field_specifiers'"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import spacy \n",
    "import nltk\n",
    "from matplotlib.pyplot import imread\n",
    "from wordcloud import WordCloud, STOPWORDS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95dc5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "Elon=pd.read_csv(\"Elon_musk.csv\",encoding='Latin-1')\n",
    "Elon.drop(['Unnamed: 0'],inplace=True,axis=1)\n",
    "Elon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762ee424",
   "metadata": {},
   "source": [
    "### Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabe3510",
   "metadata": {},
   "outputs": [],
   "source": [
    "Elon=[Text.strip() for Text in Elon.Text] # remove both the leading and the trailing characters\n",
    "Elon=[Text for Text in Elon if Text] # removes empty strings, because they are considered in Python as False\n",
    "Elon[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4ccc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joining the list into one string/text\n",
    "Elon_text=' '.join(Elon)\n",
    "Elon_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31138dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove Twitter username handles from a given twitter text. (Removes @usernames)\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer(strip_handles=True)\n",
    "Elon_tokens=tknzr.tokenize(Elon_text)\n",
    "print(Elon_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f8919e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again Joining the list into one string/text\n",
    "Elon_tokens_text=' '.join(Elon_tokens)\n",
    "Elon_tokens_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4304667f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Punctuations \n",
    "Punctuations =Elon_tokens_text.translate(str.maketrans('','',string.punctuation))\n",
    "Punctuations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643f69eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove https or url within text\n",
    "import re\n",
    "url=re.sub(r'http\\S+', '', Punctuations )\n",
    "url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7f3a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenization\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "text_tokens=word_tokenize(url)\n",
    "print(text_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9a9eb6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Tokens count\n",
    "len(text_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f16bd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Stopwords\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "my_stop_words=stopwords.words('english')\n",
    "\n",
    "sw_list = ['\\x92','rt','ye','yeah','haha','Yes','U0001F923','I']\n",
    "my_stop_words.extend(sw_list)\n",
    "\n",
    "no_stop_tokens=[word for word in text_tokens if not word in my_stop_words]\n",
    "print(no_stop_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9fb521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "lower_words=[Text.lower() for Text in no_stop_tokens]\n",
    "print(lower_words[100:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8db0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming (Optional)\n",
    "from nltk.stem import PorterStemmer\n",
    "ps=PorterStemmer()\n",
    "stemmed_tokens=[ps.stem(word) for word in lower_words]\n",
    "print(stemmed_tokens[100:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121e12e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization\n",
    "#!python -m spacy download en\n",
    "nlp=spacy.load('en_core_web_sm')\n",
    "doc=nlp(' '.join(lower_words))\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5256050e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas=[token.lemma_ for token in doc]\n",
    "print(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81207886",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_tweets=' '.join(lemmas)\n",
    "clean_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6884e28e",
   "metadata": {},
   "source": [
    "### Feature Extaction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb820bf4",
   "metadata": {},
   "source": [
    "#### 1. Using CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9741ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv=CountVectorizer()\n",
    "tweetscv=cv.fit_transform(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272c4256",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cv.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c542b682",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cv.get_feature_names()[100:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0c39a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tweetscv.toarray()[100:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea276c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tweetscv.toarray().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10de1062",
   "metadata": {},
   "source": [
    "#### 2. CountVectorizer with N-grams (Bigrams & Trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848eeba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_ngram_range=CountVectorizer(analyzer='word',ngram_range=(1,3),max_features=100)\n",
    "bow_matrix_ngram=cv_ngram_range.fit_transform(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1212fa0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cv_ngram_range.get_feature_names())\n",
    "print(bow_matrix_ngram.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37e5ddf",
   "metadata": {},
   "source": [
    "#### 3. TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d99937",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidfv_ngram_max_features=TfidfVectorizer(norm='l2',analyzer='word',ngram_range=(1,3),max_features=500)\n",
    "tfidf_matix_ngram=tfidfv_ngram_max_features.fit_transform(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab344b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tfidfv_ngram_max_features.get_feature_names())\n",
    "print(tfidf_matix_ngram.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb160c0",
   "metadata": {},
   "source": [
    "#### 4. Generate Word Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadc53b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to plot word cloud\n",
    "def plot_cloud(wordcloud):\n",
    "    plt.figure(figsize=(40,30))\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis('off')\n",
    "    \n",
    "# Generate Word Cloud\n",
    "\n",
    "STOPWORDS.add('pron')\n",
    "STOPWORDS.add('rt')\n",
    "STOPWORDS.add('yeah')\n",
    "wordcloud=WordCloud(width=3000,height=2000,background_color='black',max_words=50,\n",
    "                   colormap='Set1',stopwords=STOPWORDS).generate(clean_tweets)\n",
    "plot_cloud(wordcloud)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f4c467",
   "metadata": {},
   "source": [
    "#### 5. Named Entity Recognition (NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc8e30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parts Of Speech (POS) Tagging\n",
    "nlp=spacy.load('en_core_web_sm')\n",
    "\n",
    "one_block=clean_tweets\n",
    "doc_block=nlp(one_block)\n",
    "spacy.displacy.render(doc_block,style='ent',jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb164ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc_block[100:200]:\n",
    "    print(token,token.pos_)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f27209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering the nouns and verbs only\n",
    "nouns_verbs=[token.text for token in doc_block if token.pos_ in ('NOUN','VERB')]\n",
    "print(nouns_verbs[100:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766adb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting the noun & verb tokens\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv=CountVectorizer()\n",
    "\n",
    "X=cv.fit_transform(nouns_verbs)\n",
    "sum_words=X.sum(axis=0)\n",
    "\n",
    "words_freq=[(word,sum_words[0,idx]) for word,idx in cv.vocabulary_.items()]\n",
    "words_freq=sorted(words_freq, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "wd_df=pd.DataFrame(words_freq)\n",
    "wd_df.columns=['word','count']\n",
    "wd_df[0:10] # viewing top ten results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c29bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing results (Barchart for top 10 nouns + verbs)\n",
    "wd_df[0:10].plot.bar(x='word',figsize=(12,8),title='Top 10 nouns and verbs');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a6bc41",
   "metadata": {},
   "source": [
    "#### 6. Emotion Mining - Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6ca76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import tokenize\n",
    "sentences=tokenize.sent_tokenize(' '.join(Elon))\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6dc624",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_df=pd.DataFrame(sentences,columns=['sentence'])\n",
    "sent_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc09630",
   "metadata": {},
   "source": [
    "# second dataset Afinn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b901a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "afin = pd.read_csv(\"Afinn.csv\",  encoding='latin-1')\n",
    "afin\n",
    "#sep=',',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef68690",
   "metadata": {},
   "outputs": [],
   "source": [
    "affinity_scores=afin.set_index('word')['value'].to_dict()\n",
    "affinity_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9f6f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom function: score each word in a sentence in lemmatised form, but calculate the score for the whole original sentence\n",
    "nlp=spacy.load('en_core_web_sm')\n",
    "sentiment_lexicon=affinity_scores\n",
    "\n",
    "def calculate_sentiment(text:str=None):\n",
    "    sent_score=0\n",
    "    if text:\n",
    "        sentence=nlp(text)\n",
    "        for word in sentence:\n",
    "            sent_score+=sentiment_lexicon.get(word.lemma_,0)\n",
    "    return sent_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64495abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# manual testing\n",
    "calculate_sentiment(text='great')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e625bda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating sentiment value for each sentence\n",
    "sent_df['sentiment_value']=sent_df['sentence'].apply(calculate_sentiment)\n",
    "sent_df['sentiment_value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c607b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many words are there in a sentence?\n",
    "sent_df['word_count']=sent_df['sentence'].str.split().apply(len)\n",
    "sent_df['word_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a9b1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_df.sort_values(by='sentiment_value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5121382f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment score of the whole review\n",
    "sent_df['sentiment_value'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e29389c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# negative sentiment score of the whole review\n",
    "sent_df[sent_df['sentiment_value']<=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d98bf72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# positive sentiment score of the whole review\n",
    "sent_df[sent_df['sentiment_value']>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fbb954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding index cloumn\n",
    "sent_df['index']=range(0,len(sent_df))\n",
    "sent_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40db281d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the sentiment value for whole review\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(15,10))\n",
    "sns.distplot(sent_df['sentiment_value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69eea243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the line plot for sentiment value of whole review\n",
    "plt.figure(figsize=(15,10))\n",
    "sns.lineplot(y='sentiment_value',x='index',data=sent_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3afc165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis\n",
    "sent_df.plot.scatter(x='word_count',y='sentiment_value',figsize=(8,8),title='Sentence sentiment value to sentence word count');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
